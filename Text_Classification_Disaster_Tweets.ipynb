{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aa2020",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "Predict which Tweets are about real disasters and which ones are not.\n",
    "\n",
    "**Link to Kaggle competition:**\n",
    "Addison Howard, devrishi, Phil Culliton, Yufeng Guo. (2019). Natural Language Processing with Disaster Tweets. Kaggle. https://kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "### Contents of Notebook:\n",
    "- Defining **Customer Tokenizer with Lemmatization**\n",
    "- nltk's Tweet Tokenizer\n",
    "- Classification Method 1: **Multinomial Naive Bayes**\n",
    "- Classification Method 2: **Linear Discriminant Analysis (LDA)**\n",
    "- **TruncatedSVD** along with LDA\n",
    "- Optimal hyperparameter search through **GridSearchCV** for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b815a5f",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries & dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b14040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from nltk import tokenize as tok\n",
    "import nltk\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c035a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21bcf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327aa2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e66b58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"keyword\"].dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38391ccf",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdec7a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Our Deeds are the Reason of this #earthquake M...\n",
       "1               Forest fire near La Ronge Sask. Canada\n",
       "2    All residents asked to 'shelter in place' are ...\n",
       "3    13,000 people receive #wildfires evacuation or...\n",
       "4    Just got sent this photo from Ruby #Alaska as ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data[\"text\"]\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b080f55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       " 'Forest fire near La Ronge Sask. Canada',\n",
       " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
       " '13,000 people receive #wildfires evacuation orders in California ',\n",
       " 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus.to_list()\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8281527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(data[\"target\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "963629eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 1648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  01  04  05  06  07  08  10  100  11  ...  years  yes  yesterday  yo  \\\n",
       "0      0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "1      0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "2      0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "3      0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "4      0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ...    ...  ...        ...  ..   \n",
       "7608   0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "7609   0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "7610   0   1   1   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "7611   0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "7612   0   0   0   0   0   0   0   0    0   0  ...      0    0          0   0   \n",
       "\n",
       "      york  young  youth  youtube  yyc  zone  \n",
       "0        0      0      0        0    0     0  \n",
       "1        0      0      0        0    0     0  \n",
       "2        0      0      0        0    0     0  \n",
       "3        0      0      0        0    0     0  \n",
       "4        0      0      0        0    0     0  \n",
       "...    ...    ...    ...      ...  ...   ...  \n",
       "7608     0      0      0        0    0     0  \n",
       "7609     0      0      0        0    0     0  \n",
       "7610     0      0      0        0    0     0  \n",
       "7611     0      0      0        0    0     0  \n",
       "7612     0      0      0        0    0     0  \n",
       "\n",
       "[7613 rows x 1648 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = text.CountVectorizer(strip_accents=\"unicode\",\n",
    "                                 lowercase= True,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 analyzer=\"word\",\n",
    "                                 min_df = 0.001,\n",
    "                                 stop_words=\"english\"\n",
    "                                 )\n",
    "\n",
    "count_mat = count_vec.fit_transform(corpus).toarray()\n",
    "count_df = pd.DataFrame(count_mat)\n",
    "count_df.columns = count_vec.get_feature_names_out()\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69ec410e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reason': 1168,\n",
       " 'earthquake': 479,\n",
       " 'allah': 91,\n",
       " 'forest': 591,\n",
       " 'near': 994,\n",
       " 'la': 821,\n",
       " 'canada': 263,\n",
       " 'asked': 134,\n",
       " 'place': 1092,\n",
       " 'evacuation': 508,\n",
       " 'orders': 1043,\n",
       " 'expected': 515,\n",
       " '13': 12,\n",
       " 'people': 1073,\n",
       " 'wildfires': 1597,\n",
       " 'california': 255,\n",
       " 'just': 803,\n",
       " 'got': 642,\n",
       " 'sent': 1281,\n",
       " 'photo': 1081,\n",
       " 'alaska': 88,\n",
       " 'smoke': 1332,\n",
       " 'school': 1254,\n",
       " 'update': 1526,\n",
       " 'hwy': 748,\n",
       " '20': 22,\n",
       " 'closed': 316,\n",
       " 'lake': 826,\n",
       " 'county': 358,\n",
       " 'flood': 578,\n",
       " 'disaster': 440,\n",
       " 'heavy': 698,\n",
       " 'rain': 1154,\n",
       " 'causes': 284,\n",
       " 'flash': 574,\n",
       " 'flooding': 579,\n",
       " 'streets': 1381,\n",
       " 'colorado': 327,\n",
       " 'areas': 121,\n",
       " 'emergency': 492,\n",
       " 'happening': 676,\n",
       " 'building': 239,\n",
       " 'street': 1380,\n",
       " 'tornado': 1463,\n",
       " 'coming': 332,\n",
       " 'area': 120,\n",
       " 'died': 435,\n",
       " 'heat': 696,\n",
       " 'wave': 1575,\n",
       " 'far': 541,\n",
       " 'haha': 665,\n",
       " 'south': 1345,\n",
       " 'getting': 625,\n",
       " 'wait': 1556,\n",
       " 'second': 1266,\n",
       " 'live': 873,\n",
       " 'gonna': 639,\n",
       " 'florida': 581,\n",
       " '18': 19,\n",
       " 'days': 398,\n",
       " 've': 1541,\n",
       " 'lost': 890,\n",
       " 'myanmar': 986,\n",
       " 'arrived': 127,\n",
       " 'damage': 386,\n",
       " 'bus': 245,\n",
       " 'car': 268,\n",
       " 'crash': 365,\n",
       " 'breaking': 224,\n",
       " 'man': 911,\n",
       " 'love': 895,\n",
       " 'summer': 1394,\n",
       " 'lovely': 897,\n",
       " 'fast': 543,\n",
       " 'london': 880,\n",
       " 'cool': 351,\n",
       " 'day': 397,\n",
       " 'way': 1578,\n",
       " 'shit': 1299,\n",
       " 'nyc': 1016,\n",
       " 'week': 1585,\n",
       " 'like': 861,\n",
       " 'end': 495,\n",
       " 'ablaze': 54,\n",
       " 'http': 740,\n",
       " 'try': 1497,\n",
       " 'bring': 226,\n",
       " 'metal': 942,\n",
       " 'rt': 1222,\n",
       " 'news': 1001,\n",
       " 'flag': 572,\n",
       " 'set': 1286,\n",
       " 'aba': 51,\n",
       " 'crying': 378,\n",
       " 'plus': 1103,\n",
       " 'look': 883,\n",
       " 'sky': 1327,\n",
       " 'night': 1006,\n",
       " 'new': 1000,\n",
       " 'season': 1263,\n",
       " 'office': 1025,\n",
       " 'uo': 1525,\n",
       " 'cars': 273,\n",
       " 'cruz': 377,\n",
       " 'head': 689,\n",
       " 'st': 1357,\n",
       " 'police': 1107,\n",
       " 'lord': 887,\n",
       " 'check': 297,\n",
       " 'outside': 1048,\n",
       " 'alive': 90,\n",
       " 'dead': 400,\n",
       " 'inside': 769,\n",
       " 'awesome': 148,\n",
       " 'time': 1450,\n",
       " 'site': 1324,\n",
       " 'thanks': 1433,\n",
       " 'taking': 1413,\n",
       " 'care': 269,\n",
       " 'wanted': 1564,\n",
       " 'chicago': 299,\n",
       " 'followers': 584,\n",
       " 'know': 820,\n",
       " 'grow': 654,\n",
       " 'west': 1590,\n",
       " 'burned': 242,\n",
       " 'thousands': 1441,\n",
       " 'perfect': 1075,\n",
       " 'life': 857,\n",
       " 'leave': 845,\n",
       " 'quite': 1149,\n",
       " 'weird': 1587,\n",
       " 'better': 177,\n",
       " 'used': 1534,\n",
       " 'single': 1315,\n",
       " 'year': 1637,\n",
       " 'shot': 1303,\n",
       " 'home': 725,\n",
       " 'wife': 1594,\n",
       " 'years': 1638,\n",
       " 'setting': 1287,\n",
       " 'arsonist': 129,\n",
       " 'black': 190,\n",
       " 'church': 306,\n",
       " 'north': 1008,\n",
       " 'el': 488,\n",
       " 'happy': 678,\n",
       " 'training': 1473,\n",
       " 'hard': 679,\n",
       " 'later': 834,\n",
       " 'truck': 1492,\n",
       " 'ave': 144,\n",
       " 'city': 308,\n",
       " 'u_': 1510,\n",
       " 'https': 741,\n",
       " 'tonight': 1460,\n",
       " 'shots': 1304,\n",
       " 'climate': 314,\n",
       " 'videos': 1547,\n",
       " 'means': 927,\n",
       " 'route': 1221,\n",
       " 'month': 968,\n",
       " 'students': 1387,\n",
       " 'amp': 99,\n",
       " 'secret': 1268,\n",
       " '30': 35,\n",
       " '2013': 24,\n",
       " 'steve': 1370,\n",
       " 'fires': 571,\n",
       " 'nowplaying': 1010,\n",
       " 'edm': 485,\n",
       " 'huge': 742,\n",
       " 'does': 447,\n",
       " 'talk': 1414,\n",
       " 'don': 452,\n",
       " 'make': 906,\n",
       " 'work': 1615,\n",
       " 'kids': 808,\n",
       " 'accident': 59,\n",
       " 'michael': 945,\n",
       " '24': 30,\n",
       " 'traffic': 1470,\n",
       " 'moving': 976,\n",
       " 'center': 287,\n",
       " 'lane': 831,\n",
       " 'blocked': 199,\n",
       " 'great': 648,\n",
       " 'america': 96,\n",
       " 'read': 1163,\n",
       " 'help': 704,\n",
       " 'teen': 1420,\n",
       " 'accidents': 60,\n",
       " 'reported': 1189,\n",
       " 'vehicle': 1542,\n",
       " 'rd': 1160,\n",
       " 'involving': 782,\n",
       " 'use': 1533,\n",
       " 'mile': 948,\n",
       " '31': 37,\n",
       " 'pm': 1104,\n",
       " 'sleeping': 1328,\n",
       " 'double': 455,\n",
       " 'risk': 1209,\n",
       " 'knew': 818,\n",
       " 'happen': 674,\n",
       " '08': 6,\n",
       " '06': 4,\n",
       " '15': 15,\n",
       " '11': 9,\n",
       " '40': 40,\n",
       " 'scene': 1253,\n",
       " 'owner': 1049,\n",
       " 'mom': 965,\n",
       " 'didn': 433,\n",
       " 'horrible': 729,\n",
       " 'past': 1066,\n",
       " 'sunday': 1396,\n",
       " 'finally': 566,\n",
       " 'able': 55,\n",
       " 'thank': 1432,\n",
       " 'god': 634,\n",
       " 'tell': 1421,\n",
       " 'gt': 656,\n",
       " '23': 29,\n",
       " 'motorcyclist': 972,\n",
       " 'dies': 436,\n",
       " 'property': 1137,\n",
       " 'dr': 458,\n",
       " 'turned': 1503,\n",
       " 'left': 848,\n",
       " 'manchester': 912,\n",
       " 'stop': 1372,\n",
       " '2015': 26,\n",
       " 'injury': 767,\n",
       " 'fatal': 545,\n",
       " 'right': 1204,\n",
       " 'exit': 513,\n",
       " 'nc': 993,\n",
       " '16': 16,\n",
       " 'support': 1399,\n",
       " 'plans': 1096,\n",
       " 'going': 636,\n",
       " 'deadly': 401,\n",
       " 'happened': 675,\n",
       " 'today': 1455,\n",
       " 'll': 876,\n",
       " 'fucking': 604,\n",
       " 'drive': 463,\n",
       " 'road': 1213,\n",
       " 'killed': 810,\n",
       " 'explosion': 521,\n",
       " 'heard': 693,\n",
       " 'comment': 333,\n",
       " 'issue': 793,\n",
       " 'game': 612,\n",
       " 'gets': 624,\n",
       " 'win': 1598,\n",
       " 'aftershock': 75,\n",
       " 'dubstep': 472,\n",
       " 'dnb': 446,\n",
       " 'dance': 390,\n",
       " 'david': 396,\n",
       " 'came': 260,\n",
       " 'im': 754,\n",
       " 'using': 1537,\n",
       " 'shooting': 1300,\n",
       " 'guess': 658,\n",
       " 'actually': 67,\n",
       " 'wants': 1565,\n",
       " 'free': 597,\n",
       " 'best': 176,\n",
       " 'seeing': 1271,\n",
       " 'issues': 795,\n",
       " 'minute': 956,\n",
       " 'daily': 385,\n",
       " 'really': 1167,\n",
       " 'global': 632,\n",
       " 'financial': 567,\n",
       " 'meltdown': 936,\n",
       " 'moment': 966,\n",
       " 'guy': 663,\n",
       " 'screaming': 1257,\n",
       " 'bloody': 202,\n",
       " 'murder': 982,\n",
       " 'youtube': 1645,\n",
       " 'book': 215,\n",
       " 'face': 528,\n",
       " 'doing': 451,\n",
       " 'wrong': 1632,\n",
       " 'thing': 1435,\n",
       " 'dream': 460,\n",
       " 'possible': 1113,\n",
       " 'brown': 234,\n",
       " 'die': 434,\n",
       " 'avoid': 145,\n",
       " 'thinking': 1438,\n",
       " 'lose': 888,\n",
       " 'jobs': 799,\n",
       " 'tried': 1487,\n",
       " 'kick': 806,\n",
       " 'want': 1563,\n",
       " 'making': 908,\n",
       " 'say': 1249,\n",
       " 'need': 997,\n",
       " 'play': 1097,\n",
       " 'experts': 517,\n",
       " 'france': 595,\n",
       " 'begin': 173,\n",
       " 'examining': 511,\n",
       " 'airplane': 84,\n",
       " 'debris': 406,\n",
       " 'reunion': 1199,\n",
       " 'island': 789,\n",
       " 'french': 598,\n",
       " 'air': 81,\n",
       " 'pilot': 1090,\n",
       " 'common': 334,\n",
       " 'dying': 475,\n",
       " 'good': 640,\n",
       " 'job': 798,\n",
       " 'family': 536,\n",
       " 'members': 937,\n",
       " 'bin': 184,\n",
       " 'laden': 823,\n",
       " 'gov': 644,\n",
       " 'suspect': 1405,\n",
       " 'goes': 635,\n",
       " '29': 33,\n",
       " '07': 5,\n",
       " 'men': 939,\n",
       " 'including': 759,\n",
       " 'state': 1366,\n",
       " 'government': 645,\n",
       " 'official': 1027,\n",
       " 'wednesday': 1584,\n",
       " 'began': 172,\n",
       " 'rip': 1207,\n",
       " 'mode': 963,\n",
       " 'wreck': 1629,\n",
       " 'politics': 1109,\n",
       " 'airport': 85,\n",
       " 'aircraft': 82,\n",
       " 'plane': 1094,\n",
       " 'death': 403,\n",
       " 'wtf': 1634,\n",
       " 'uat': 1513,\n",
       " 'believe': 174,\n",
       " 'eyes': 526,\n",
       " 'victim': 1544,\n",
       " 'crashed': 366,\n",
       " 'times': 1452,\n",
       " 'ago': 76,\n",
       " 'little': 872,\n",
       " 'bit': 188,\n",
       " 'trauma': 1478,\n",
       " 'omg': 1036,\n",
       " 'bro': 228,\n",
       " 'phone': 1080,\n",
       " 'looks': 885,\n",
       " 'ship': 1297,\n",
       " 'cop': 352,\n",
       " 'house': 738,\n",
       " '12': 10,\n",
       " 'cause': 282,\n",
       " 'close': 315,\n",
       " 'early': 477,\n",
       " 'wake': 1558,\n",
       " 'sister': 1322,\n",
       " 'come': 330,\n",
       " 'ambulance': 95,\n",
       " 'hospital': 732,\n",
       " 'feared': 551,\n",
       " 'pakistani': 1052,\n",
       " 'helicopter': 700,\n",
       " 'reuters': 1200,\n",
       " 'leading': 840,\n",
       " 'services': 1285,\n",
       " 'charity': 296,\n",
       " 'incident': 758,\n",
       " '14': 14,\n",
       " 'ebay': 482,\n",
       " 'target': 1416,\n",
       " 'destroy': 423,\n",
       " 'blood': 201,\n",
       " 'crazy': 367,\n",
       " 'couple': 359,\n",
       " 'run': 1228,\n",
       " 'lucky': 901,\n",
       " 'waiting': 1557,\n",
       " 'ok': 1032,\n",
       " 'pakistan': 1051,\n",
       " 'kills': 813,\n",
       " 'ny': 1015,\n",
       " 'petition': 1077,\n",
       " '17': 18,\n",
       " 'hour': 736,\n",
       " 'ua': 1511,\n",
       " 'lot': 891,\n",
       " 'said': 1236,\n",
       " 'lt': 900,\n",
       " 'ui': 1516,\n",
       " 'dog': 449,\n",
       " 'sirens': 1320,\n",
       " 'worldnews': 1619,\n",
       " 'number': 1012,\n",
       " 'body': 208,\n",
       " 'practice': 1120,\n",
       " 'trust': 1495,\n",
       " 'walk': 1559,\n",
       " 'hate': 683,\n",
       " 'episode': 503,\n",
       " 'annihilated': 104,\n",
       " 'nigga': 1004,\n",
       " 'shall': 1291,\n",
       " 'hey': 707,\n",
       " 'meeting': 935,\n",
       " 'career': 270,\n",
       " 'easy': 481,\n",
       " 'sorry': 1340,\n",
       " 'won': 1610,\n",
       " 'drunk': 471,\n",
       " 'driver': 464,\n",
       " 'safety': 1235,\n",
       " 'seconds': 1267,\n",
       " 'hit': 718,\n",
       " 'train': 1472,\n",
       " 'country': 357,\n",
       " 'sure': 1401,\n",
       " 'israel': 791,\n",
       " 'horror': 730,\n",
       " 'iran': 783,\n",
       " 'spent': 1350,\n",
       " 'history': 717,\n",
       " '70': 48,\n",
       " 'humanity': 744,\n",
       " 'went': 1589,\n",
       " 'fact': 529,\n",
       " 'stopped': 1373,\n",
       " 'quickly': 1148,\n",
       " 'short': 1302,\n",
       " 'ball': 155,\n",
       " 'ready': 1165,\n",
       " 'weather': 1582,\n",
       " 'thought': 1440,\n",
       " 'forecast': 590,\n",
       " 'feat': 553,\n",
       " 'seen': 1274,\n",
       " 'case': 274,\n",
       " 'completely': 339,\n",
       " 'paul': 1069,\n",
       " 'survivors': 1404,\n",
       " 'match': 921,\n",
       " 'syrian': 1408,\n",
       " 'army': 125,\n",
       " 'pile': 1089,\n",
       " 'wasn': 1570,\n",
       " 'food': 586,\n",
       " 'bc': 167,\n",
       " 'fun': 607,\n",
       " 'bar': 159,\n",
       " 'hell': 701,\n",
       " 'total': 1464,\n",
       " 'annihilation': 105,\n",
       " 'destruction': 426,\n",
       " 'usa': 1531,\n",
       " 'potus': 1117,\n",
       " 'maybe': 923,\n",
       " 'river': 1210,\n",
       " 'national': 989,\n",
       " 'park': 1060,\n",
       " 'tonto': 1461,\n",
       " 'salt': 1239,\n",
       " 'wild': 1595,\n",
       " 'horse': 731,\n",
       " 'change': 292,\n",
       " 'world': 1618,\n",
       " 'vs': 1554,\n",
       " 'self': 1276,\n",
       " 'attack': 137,\n",
       " 'sign': 1311,\n",
       " 'share': 1294,\n",
       " 'save': 1245,\n",
       " 'thx': 1447,\n",
       " 'mention': 940,\n",
       " 'major': 905,\n",
       " 'stand': 1360,\n",
       " 'false': 534,\n",
       " 'hours': 737,\n",
       " 'away': 147,\n",
       " 'long': 881,\n",
       " 'order': 1042,\n",
       " 'books': 216,\n",
       " '2011': 23,\n",
       " 'join': 801,\n",
       " 'following': 585,\n",
       " 'zone': 1647,\n",
       " 'fight': 562,\n",
       " 'soon': 1339,\n",
       " 'survive': 1402,\n",
       " 'apocalypse': 114,\n",
       " 'feels': 558,\n",
       " 'poor': 1111,\n",
       " 'boy': 220,\n",
       " 'child': 301,\n",
       " 'birthday': 187,\n",
       " 'watch': 1571,\n",
       " 'human': 743,\n",
       " 'film': 564,\n",
       " 'august': 141,\n",
       " '05': 3,\n",
       " 'red': 1172,\n",
       " '2014': 25,\n",
       " 'feel': 556,\n",
       " 'kinda': 814,\n",
       " 'hot': 735,\n",
       " 'played': 1098,\n",
       " 'radio': 1152,\n",
       " 'disease': 443,\n",
       " 'started': 1364,\n",
       " 'careful': 271,\n",
       " 'lol': 879,\n",
       " 'question': 1147,\n",
       " 'called': 256,\n",
       " 'version': 1543,\n",
       " 'block': 198,\n",
       " 'mod': 962,\n",
       " 'snow': 1333,\n",
       " 'reading': 1164,\n",
       " '21': 27,\n",
       " 'angel': 101,\n",
       " 'took': 1462,\n",
       " 'high': 709,\n",
       " 'mountain': 974,\n",
       " 'imagine': 756,\n",
       " 'fighting': 563,\n",
       " 'heart': 695,\n",
       " 'liked': 862,\n",
       " 'video': 1546,\n",
       " 'dad': 384,\n",
       " 'bought': 217,\n",
       " 'science': 1255,\n",
       " 'comes': 331,\n",
       " 'movie': 975,\n",
       " 'scared': 1252,\n",
       " 'storm': 1375,\n",
       " 'latest': 835,\n",
       " 'totally': 1466,\n",
       " 'bad': 151,\n",
       " 'dark': 393,\n",
       " 'queen': 1146,\n",
       " 'action': 65,\n",
       " '300': 36,\n",
       " 'armageddon': 124,\n",
       " 'kill': 809,\n",
       " 'peace': 1072,\n",
       " 'bed': 171,\n",
       " 'unless': 1524,\n",
       " 'start': 1363,\n",
       " 'series': 1282,\n",
       " 'beat': 169,\n",
       " 'ben': 175,\n",
       " 'girls': 628,\n",
       " 'loved': 896,\n",
       " 'hand': 670,\n",
       " 'sense': 1278,\n",
       " 'door': 454,\n",
       " 'data': 394,\n",
       " 'blog': 200,\n",
       " 'did': 432,\n",
       " 'based': 160,\n",
       " 'tomorrow': 1458,\n",
       " 'points': 1106,\n",
       " 'department': 417,\n",
       " 'haven': 684,\n",
       " 'putin': 1142,\n",
       " 'warning': 1567,\n",
       " 'late': 833,\n",
       " 'escape': 505,\n",
       " 'heavenly': 697,\n",
       " 'rule': 1226,\n",
       " 'earth': 478,\n",
       " 'let': 851,\n",
       " 'uas': 1512,\n",
       " 'stage': 1359,\n",
       " 'toddler': 1456,\n",
       " 'yesterday': 1640,\n",
       " 'saw': 1248,\n",
       " 'hail': 666,\n",
       " 'think': 1437,\n",
       " 'sadly': 1234,\n",
       " '10': 7,\n",
       " 'extremely': 524,\n",
       " 'till': 1449,\n",
       " 'united': 1522,\n",
       " 'working': 1617,\n",
       " 'class': 312,\n",
       " 'prepare': 1125,\n",
       " 'crisis': 373,\n",
       " 'economic': 484,\n",
       " 'collapse': 322,\n",
       " 'tracks': 1469,\n",
       " 'russia': 1231,\n",
       " 'isn': 790,\n",
       " 'funny': 608,\n",
       " 'beyonce': 179,\n",
       " 'pick': 1084,\n",
       " 'fan': 538,\n",
       " 'beyhive': 178,\n",
       " 'direction': 438,\n",
       " 'directioners': 439,\n",
       " '22': 28,\n",
       " 'build': 238,\n",
       " 'lead': 838,\n",
       " 'friend': 600,\n",
       " 'children': 302,\n",
       " 'vote': 1553,\n",
       " 'round': 1220,\n",
       " '100': 8,\n",
       " 'dogs': 450,\n",
       " 'leader': 839,\n",
       " 'hero': 706,\n",
       " 'tv': 1504,\n",
       " 'review': 1201,\n",
       " 'blue': 204,\n",
       " 'rea': 1161,\n",
       " 'date': 395,\n",
       " 'women': 1608,\n",
       " 'rubber': 1223,\n",
       " 'indian': 761,\n",
       " 'da': 383,\n",
       " 'hope': 728,\n",
       " 'leads': 841,\n",
       " 'wwii': 1635,\n",
       " 'japanese': 797,\n",
       " 'navy': 992,\n",
       " 'military': 950,\n",
       " 'japan': 796,\n",
       " 'leather': 844,\n",
       " 'war': 1566,\n",
       " 'learn': 842,\n",
       " 'ap': 112,\n",
       " 'violent': 1550,\n",
       " 'control': 349,\n",
       " 'terrorists': 1428,\n",
       " 'charged': 295,\n",
       " 'arson': 128,\n",
       " 'truth': 1496,\n",
       " 'cases': 275,\n",
       " 'squad': 1356,\n",
       " 'crime': 372,\n",
       " 'burn': 241,\n",
       " 'linked': 868,\n",
       " 'caught': 281,\n",
       " 'northern': 1009,\n",
       " 'victims': 1545,\n",
       " 'sound': 1343,\n",
       " 'gay': 616,\n",
       " 'palestinian': 1053,\n",
       " 'amid': 98,\n",
       " 'girl': 627,\n",
       " 'destroyed': 424,\n",
       " 'white': 1593,\n",
       " 'east': 480,\n",
       " 'bay': 164,\n",
       " 'arrested': 126,\n",
       " 'suspected': 1406,\n",
       " 'nice': 1002,\n",
       " 'sit': 1323,\n",
       " 'shows': 1308,\n",
       " 'captures': 267,\n",
       " 'american': 97,\n",
       " 'beach': 168,\n",
       " 'ca': 251,\n",
       " 'burning': 243,\n",
       " 'things': 1436,\n",
       " 'makes': 907,\n",
       " 'town': 1468,\n",
       " 'ice': 749,\n",
       " 'blaze': 192,\n",
       " 'business': 248,\n",
       " 'reports': 1190,\n",
       " 'fuck': 603,\n",
       " 'drink': 461,\n",
       " 'green': 650,\n",
       " 'court': 361,\n",
       " 'lmao': 877,\n",
       " 'real': 1166,\n",
       " 'hiring': 715,\n",
       " 'terrorism': 1426,\n",
       " 'big': 181,\n",
       " 'true': 1493,\n",
       " 'story': 1376,\n",
       " 'missing': 960,\n",
       " 'stay': 1368,\n",
       " 'theater': 1434,\n",
       " 'gun': 660,\n",
       " 'terror': 1425,\n",
       " 'post': 1114,\n",
       " 'udhampur': 1515,\n",
       " 'million': 951,\n",
       " 'nuclear': 1011,\n",
       " 'claims': 311,\n",
       " 'suicide': 1393,\n",
       " 'bombing': 212,\n",
       " 'saudi': 1244,\n",
       " 'mosque': 971,\n",
       " 'twitter': 1508,\n",
       " 'militants': 949,\n",
       " 'spos': 1353,\n",
       " 'injured': 765,\n",
       " 'obama': 1018,\n",
       " 'officials': 1028,\n",
       " 'gave': 615,\n",
       " 'terrorist': 1427,\n",
       " 'weapon': 1580,\n",
       " 'texas': 1430,\n",
       " 'user': 1535,\n",
       " 'internet': 776,\n",
       " 'israeli': 792,\n",
       " 'worry': 1621,\n",
       " 'okay': 1033,\n",
       " 'lie': 855,\n",
       " 'shift': 1296,\n",
       " 'ii': 752,\n",
       " 'response': 1196,\n",
       " 'yeah': 1636,\n",
       " 'damn': 388,\n",
       " 'india': 760,\n",
       " 'illegal': 753,\n",
       " 'released': 1180,\n",
       " 'woman': 1607,\n",
       " 'team': 1417,\n",
       " 'act': 64,\n",
       " 'trying': 1498,\n",
       " 'atomic': 136,\n",
       " 'bombs': 214,\n",
       " 'fat': 544,\n",
       " 'says': 1251,\n",
       " 'group': 653,\n",
       " 'gop': 641,\n",
       " 'attacked': 138,\n",
       " 'feeling': 557,\n",
       " 'media': 930,\n",
       " 'civilians': 310,\n",
       " 'christian': 305,\n",
       " 'muslims': 985,\n",
       " 'temple': 1422,\n",
       " 'mount': 973,\n",
       " 'waving': 1577,\n",
       " 'pamela': 1054,\n",
       " 'geller': 618,\n",
       " 'youth': 1644,\n",
       " 'remember': 1183,\n",
       " 'literally': 871,\n",
       " 'families': 535,\n",
       " 'worst': 1623,\n",
       " 'person': 1076,\n",
       " 'guys': 664,\n",
       " 'trump': 1494,\n",
       " 'view': 1548,\n",
       " 'needs': 998,\n",
       " 'answer': 107,\n",
       " 'program': 1134,\n",
       " 'fukushima': 605,\n",
       " 'avalanche': 143,\n",
       " 'louis': 894,\n",
       " 'box': 219,\n",
       " 'piece': 1088,\n",
       " 'album': 89,\n",
       " 'music': 984,\n",
       " 'beautiful': 170,\n",
       " 'lots': 892,\n",
       " 'calgary': 254,\n",
       " 'flames': 573,\n",
       " 'secrets': 1269,\n",
       " 'inner': 768,\n",
       " 'worth': 1624,\n",
       " 'fall': 531,\n",
       " 'fully': 606,\n",
       " 'original': 1044,\n",
       " 'em': 491,\n",
       " 'gas': 614,\n",
       " 'driving': 465,\n",
       " 'having': 685,\n",
       " 'lifted': 858,\n",
       " '4x4': 42,\n",
       " 'favorite': 549,\n",
       " 'deal': 402,\n",
       " 'yo': 1641,\n",
       " 'listen': 870,\n",
       " 'star': 1362,\n",
       " 'wars': 1568,\n",
       " 'power': 1118,\n",
       " 'battle': 162,\n",
       " 'general': 620,\n",
       " '2nd': 34,\n",
       " 'signed': 1312,\n",
       " 'added': 68,\n",
       " 'playlist': 1101,\n",
       " 'hits': 719,\n",
       " 'eye': 525,\n",
       " 'space': 1346,\n",
       " 'occurred': 1022,\n",
       " 'fleets': 576,\n",
       " 'totaling': 1465,\n",
       " 'ships': 1298,\n",
       " 'taken': 1411,\n",
       " 'king': 816,\n",
       " 'happens': 677,\n",
       " 'young': 1643,\n",
       " 'dc': 399,\n",
       " 'mass': 918,\n",
       " 'games': 613,\n",
       " 'playing': 1100,\n",
       " 'o784': 1017,\n",
       " '50': 43,\n",
       " 'australia': 142,\n",
       " 'miss': 959,\n",
       " 'son': 1337,\n",
       " 'fedex': 554,\n",
       " 'longer': 882,\n",
       " 'transport': 1476,\n",
       " 'bioterror': 185,\n",
       " 'germs': 623,\n",
       " 'anthrax': 108,\n",
       " 'lab': 822,\n",
       " 'mishaps': 958,\n",
       " 'usatoday': 1532,\n",
       " 'pathogens': 1067,\n",
       " 'potential': 1116,\n",
       " 'research': 1194,\n",
       " 'doesn': 448,\n",
       " 'cut': 381,\n",
       " 'foxnews': 594,\n",
       " 'hold': 722,\n",
       " 'hearing': 694,\n",
       " 'problem': 1133,\n",
       " 'fear': 550,\n",
       " 'bioterrorism': 186,\n",
       " 'sir': 1318,\n",
       " 'region': 1177,\n",
       " 'hostage': 733,\n",
       " 'cover': 362,\n",
       " 'public': 1139,\n",
       " 'health': 691,\n",
       " 'security': 1270,\n",
       " 'lies': 856,\n",
       " 'hollywood': 724,\n",
       " 'yes': 1639,\n",
       " 'preparedness': 1126,\n",
       " 'running': 1229,\n",
       " 'pretty': 1130,\n",
       " 'list': 869,\n",
       " 'gm': 633,\n",
       " 'special': 1348,\n",
       " 'ebola': 483,\n",
       " 'future': 610,\n",
       " 'abc': 53,\n",
       " 'irandeal': 784,\n",
       " 'broken': 230,\n",
       " 'threat': 1442,\n",
       " 'fears': 552,\n",
       " 'ahead': 79,\n",
       " 'test': 1429,\n",
       " 'event': 510,\n",
       " 'pool': 1110,\n",
       " 'looking': 884,\n",
       " 'niggas': 1005,\n",
       " 'wanna': 1562,\n",
       " 'welcome': 1588,\n",
       " 'living': 875,\n",
       " 'hair': 668,\n",
       " 'weekend': 1586,\n",
       " 'wildfire': 1596,\n",
       " 'nearly': 996,\n",
       " 'old': 1035,\n",
       " 'firefighters': 570,\n",
       " 'months': 969,\n",
       " 'pic': 1083,\n",
       " 'computers': 340,\n",
       " 'mad': 902,\n",
       " 'dont': 453,\n",
       " 'seek': 1272,\n",
       " 'experience': 516,\n",
       " 'tweet': 1505,\n",
       " 'sale': 1238,\n",
       " 'guns': 662,\n",
       " 'agree': 77,\n",
       " 'blazing': 193,\n",
       " 'party': 1063,\n",
       " 'silver': 1314,\n",
       " 'online': 1038,\n",
       " 'turn': 1502,\n",
       " 'weapons': 1581,\n",
       " 'expect': 514,\n",
       " 'lady': 825,\n",
       " 'oh': 1030,\n",
       " 'word': 1613,\n",
       " 'cold': 321,\n",
       " 'sun': 1395,\n",
       " 'rn': 1212,\n",
       " 'words': 1614,\n",
       " 'dude': 473,\n",
       " 'fantasy': 540,\n",
       " 'bag': 152,\n",
       " 'song': 1338,\n",
       " 'rare': 1157,\n",
       " 'proof': 1136,\n",
       " 'complete': 338,\n",
       " 'giving': 629,\n",
       " 'bitch': 189,\n",
       " 'cake': 253,\n",
       " 'follow': 583,\n",
       " 'ways': 1579,\n",
       " 'bleeding': 194,\n",
       " 'joe': 800,\n",
       " 'text': 1431,\n",
       " 'apparently': 117,\n",
       " 'fine': 568,\n",
       " 'walking': 1560,\n",
       " 'hands': 673,\n",
       " 'tears': 1418,\n",
       " 'glass': 631,\n",
       " 'pain': 1050,\n",
       " 'feet': 559,\n",
       " 'starts': 1365,\n",
       " 'brain': 222,\n",
       " 'lets': 852,\n",
       " 'wedding': 1583,\n",
       " 'blew': 195,\n",
       " 'bomb': 209,\n",
       " 'line': 866,\n",
       " 'entire': 499,\n",
       " 'wont': 1612,\n",
       " '90': 50,\n",
       " 'told': 1457,\n",
       " 'brought': 233,\n",
       " 'wind': 1599,\n",
       " '1st': 21,\n",
       " 'light': 859,\n",
       " 'talking': 1415,\n",
       " 'kid': 807,\n",
       " 'watched': 1572,\n",
       " 'falling': 532,\n",
       " 'blight': 196,\n",
       " 'ones': 1037,\n",
       " 'soul': 1342,\n",
       " 'searching': 1262,\n",
       " 'deep': 409,\n",
       " 'ain': 80,\n",
       " 'loving': 898,\n",
       " 'info': 763,\n",
       " 'anti': 109,\n",
       " 'releases': 1181,\n",
       " 'level': 853,\n",
       " 'stories': 1374,\n",
       " 'result': 1197,\n",
       " 'app': 116,\n",
       " 'community': 335,\n",
       " 'policy': 1108,\n",
       " 'report': 1188,\n",
       " 'open': 1039,\n",
       " 'tragedy': 1471,\n",
       " 'shame': 1292,\n",
       " 'metro': 943,\n",
       " 'art': 130,\n",
       " 'amazon': 94,\n",
       " 'buy': 250,\n",
       " 'blizzard': 197,\n",
       " 'pay': 1070,\n",
       " 'rock': 1214,\n",
       " 'hi': 708,\n",
       " 'aren': 122,\n",
       " 'leaving': 846,\n",
       " 'link': 867,\n",
       " 'bout': 218,\n",
       " 'tho': 1439,\n",
       " 'market': 916,\n",
       " 'technology': 1419,\n",
       " 'biggest': 183,\n",
       " 'bruh': 236,\n",
       " 'nah': 988,\n",
       " 'cook': 350,\n",
       " 'gone': 638,\n",
       " 'wall': 1561,\n",
       " 'broke': 229,\n",
       " 'morning': 970,\n",
       " 'ah': 78,\n",
       " 'kindle': 815,\n",
       " 'wounded': 1626,\n",
       " 'headed': 690,\n",
       " 'mean': 926,\n",
       " 'smh': 1331,\n",
       " 'pressure': 1129,\n",
       " 'standard': 1361,\n",
       " 'large': 832,\n",
       " 'related': 1178,\n",
       " 'super': 1398,\n",
       " 'seriously': 1283,\n",
       " 'drown': 468,\n",
       " ...}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the vocab\n",
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "04a8bab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.39463663, 0.41881732, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at tfidf vectors\n",
    "\n",
    "tfidf = text.TfidfTransformer(norm='l2')\n",
    "\n",
    "tfidf_mat = tfidf.fit_transform(count_df).toarray()\n",
    "tfidf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed1daa",
   "metadata": {},
   "source": [
    "### 3. Model Building & Selection\n",
    "\n",
    "#### 3.1. Getting a baseline score with NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ffcf3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "X = data[\"text\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4ffa1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classification pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[(\"count_vec\" , text.CountVectorizer(strip_accents=\"unicode\",\n",
    "                                                             lowercase= True,\n",
    "                                                             ngram_range=(1,2),\n",
    "                                                             analyzer=\"word\",\n",
    "                                                             min_df = 0.001,\n",
    "                                                           #  stop_words=\"english\",\n",
    "                                                            tokenizer=myTokenizer,\n",
    "                                                            token_pattern=None\n",
    "                                                             )),\n",
    "                       (\"tfidf\"     ,  text.TfidfTransformer(norm='l2')),\n",
    "                       (\"classifier\",  nb.MultinomialNB())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "1401b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.7729618163054696\n",
      "Test f1 score: 0.7288317256162915\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the whole train dataset\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train = pipe.predict(X_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Train f1 score:\", metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Test f1 score:\", metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e9c28",
   "metadata": {},
   "source": [
    "### 4. Adding a customer tokenizer & lemmatization\n",
    "-  **1. Lemmatizing Tokenizer (Custom-defined)**\n",
    "-  **2. nltk's TweetTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "91a3ffc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'Deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'Reason',\n",
       " 'of',\n",
       " 'this',\n",
       " '#earthquake',\n",
       " 'May',\n",
       " 'ALLAH',\n",
       " 'Forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wordnet_pos(term):\n",
    "    tag = nltk.pos_tag([term])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def myTokenizer(doc):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemm_tokens=[]\n",
    "    tokens = [term for term in tok.word_tokenize(doc) if len(term)>2]\n",
    "    lemm_tokens += [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    return lemm_tokens\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "twt_tokenizer = TweetTokenizer()\n",
    "\n",
    "twt_tokenizer.tokenize(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7c63c419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\n",
      " @mickinyman @TheAtlantic That or they might be killed in an airplane accident in the night a car wreck! Politics at it's best. \n",
      "\n",
      "\n",
      "Tokenized & lemmatized text:\n",
      "\n",
      " ['mickinyman', 'TheAtlantic', 'That', 'they', 'might', 'kill', 'airplane', 'accident', 'the', 'night', 'car', 'wreck', 'Politics', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "\n",
    "print(\"Original text:\\n\\n\", data[\"text\"][150],\"\\n\\n\")\n",
    "print(\"Tokenized & lemmatized text:\\n\\n\",myTokenizer(data[\"text\"][150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0e16cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train k-fold f1 score: 0.7204780145116031\n",
      "Train f1 score: 0.7765180146132526\n",
      "Test f1 score: 0.735632183908046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1545"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build classification pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[(\"count_vec\" , text.CountVectorizer(strip_accents=\"unicode\",\n",
    "                                                             lowercase= True,\n",
    "                                                             ngram_range=(1,1),\n",
    "                                                             analyzer=\"word\",\n",
    "                                                             min_df = 0.001,\n",
    "                                                          #  stop_words=\"english\",\n",
    "                                                             tokenizer = myTokenizer,\n",
    "                                                             token_pattern=None\n",
    "                                                             )),\n",
    "                       (\"tfidf\"     ,  text.TfidfTransformer(norm='l2')),\n",
    "                       (\"classifier\",  nb.MultinomialNB())])\n",
    "\n",
    "\n",
    "print(\"Train k-fold f1 score:\",cross_val_score(pipe, X_train, y_train, scoring = \"f1\", cv = 5).mean())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = pipe.predict(X_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Train f1 score:\", metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Test f1 score:\", metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "len(pipe.named_steps[\"tfidf\"].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94937f",
   "metadata": {},
   "source": [
    "### 4. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1e86f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train k-fold f1 score: 0.6860623563900089\n",
      "Train f1 score: 0.7055016181229773\n",
      "Test f1 score: 0.672182006204757\n",
      "2364\n"
     ]
    }
   ],
   "source": [
    "# Build classification pipeline\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def transform_to_array(X):\n",
    "    return X.toarray()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"count_vec\" , text.CountVectorizer(strip_accents=\"unicode\",\n",
    "                                                             lowercase= True,\n",
    "                                                             ngram_range=(1,2),\n",
    "                                                             analyzer=\"word\",\n",
    "                                                             min_df = 0.001,\n",
    "                                                         #   stop_words=\"english\",\n",
    "                                                             tokenizer = myTokenizer,\n",
    "                                                             token_pattern=None\n",
    "                                                             )),\n",
    "                       (\"tfidf\"     ,  text.TfidfTransformer(norm='l2')),\n",
    "                    #  (\"toarray\"  ,  FunctionTransformer(transform_to_array)),\n",
    "                       (\"dim_red\"  ,  TruncatedSVD(n_components=100)),\n",
    "                       (\"classifier\",  LinearDiscriminantAnalysis(n_components=1))])\n",
    "\n",
    "\n",
    "print(\"Train k-fold f1 score:\",cross_val_score(pipe, X_train, y_train, scoring = \"f1\", cv = 5).mean())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = pipe.predict(X_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Train f1 score:\", metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Test f1 score:\", metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "print(len(pipe.named_steps[\"tfidf\"].get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e04767",
   "metadata": {},
   "source": [
    "### 5.  LDA with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "c6243b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "e10bd49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "\n",
      "Grid best estimator:  Pipeline(steps=[('count_vec',\n",
      "                 CountVectorizer(min_df=0.001, ngram_range=(1, 2),\n",
      "                                 strip_accents='unicode', token_pattern=None,\n",
      "                                 tokenizer=<function myTokenizer at 0x2c9b3c940>)),\n",
      "                ('tfidf', TfidfTransformer()),\n",
      "                ('dim_red', TruncatedSVD(n_components=400)),\n",
      "                ('classifier', LinearDiscriminantAnalysis(n_components=1))])\n",
      "\n",
      "\n",
      "Grid best params:  {'count_vec__ngram_range': (1, 2), 'count_vec__tokenizer': <function myTokenizer at 0x2c9b3c940>, 'dim_red__n_components': 400}\n",
      "\n",
      "\n",
      "Grid best score:  0.740596572923463\n",
      "\n",
      "\n",
      "Test f1 score: 0.7427993936331481\n",
      "\n",
      "\n",
      "Train f1 score: 0.7800289435600579\n",
      "\n",
      "\n",
      "Time:  0:07:23.691764\n"
     ]
    }
   ],
   "source": [
    "start = dt.now()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"count_vec\" , text.CountVectorizer(strip_accents=\"unicode\",\n",
    "                                                             lowercase= True,\n",
    "                                                             ngram_range=(1,2),\n",
    "                                                             analyzer=\"word\",\n",
    "                                                             min_df = 0.001,\n",
    "                                                         #   stop_words=\"english\",\n",
    "                                                             tokenizer = myTokenizer,\n",
    "                                                             token_pattern=None\n",
    "                                                             )),\n",
    "                       (\"tfidf\"     ,  text.TfidfTransformer(norm='l2')),\n",
    "                       (\"dim_red\"  ,  TruncatedSVD(n_components=500)),\n",
    "                       (\"classifier\",  LinearDiscriminantAnalysis(n_components=1))])\n",
    "\n",
    "\n",
    "param_grid = {\"dim_red__n_components\":[400, 500, 600],\n",
    "              \"count_vec__ngram_range\": [(1,1), (1,2), (2,2)],\n",
    "              \"count_vec__tokenizer\": [myTokenizer, twt_tokenizer.tokenize]}\n",
    "\n",
    "grid = GridSearchCV(pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                    scoring=\"f1\",\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"\\n\\nGrid best estimator: \",grid.best_estimator_)\n",
    "print(\"\\n\\nGrid best params: \",grid.best_params_)\n",
    "print(\"\\n\\nGrid best score: \",grid.best_score_)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"\\n\\nTest f1 score:\", metrics.f1_score(y_test, y_pred))\n",
    "y_pred_train = grid.predict(X_train)\n",
    "print(\"\\n\\nTrain f1 score:\", metrics.f1_score(y_train, y_pred_train))\n",
    "\n",
    "print(\"\\n\\nTime: \", (dt.now() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0da54f",
   "metadata": {},
   "source": [
    "### 5. Get preds and submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "c2065f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[(\"count_vec\" , text.CountVectorizer(strip_accents=\"unicode\",\n",
    "                                                             lowercase= True,\n",
    "                                                             ngram_range=(1,2),\n",
    "                                                             analyzer=\"word\",\n",
    "                                                             min_df = 0.001,\n",
    "                                                         #   stop_words=\"english\",\n",
    "                                                             tokenizer = myTokenizer,\n",
    "                                                             token_pattern=None\n",
    "                                                             )),\n",
    "                       (\"tfidf\"     ,  text.TfidfTransformer(norm='l2')),\n",
    "                       (\"dim_red\"  ,  TruncatedSVD(n_components=500)),\n",
    "                       (\"classifier\",  LinearDiscriminantAnalysis(n_components=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "5d885f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data = pd.read_csv(\"test.csv\")\n",
    "X_ = test_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "9591baf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)\n",
    "y_submission_simple_LDA = pipe.predict(X_)\n",
    "y_submission_simple_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7c14e5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "id        \n",
       "0        1\n",
       "2        0\n",
       "3        1\n",
       "9        1\n",
       "11       1"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({#index: test_data[\"id\"],\n",
    "                              \"target\": y_submission_simple_LDA},\n",
    "                            index = test_data[\"id\"])\n",
    "\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "22c1e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"Submission_LDA_truSVD500_myTok.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
